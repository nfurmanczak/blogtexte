---
title: "Erfahrungsbericht LLMs"
date: 2025-01-28T20:35:41+02:00
draft: false
tags: ["AI", "LLM", "ChatGPT"]
ShowToc: true
---

---
Mittlerweile sind mehrere große LLMs (Large Language Models) von verschiedenen Anbietern seit einigen Jahren für die breite Masse verfügbar. Es ist also Zeit für ein erstes Fazit.

ChatGPT – wenn auch aktuell das bekannteste LLM – ist nicht das erste seiner Art. Bereits 2017/2018 gab es von Google und Meta (damals noch Facebook) theoretische Konzepte und erste LLMs wie ALBERT (Google) oder RoBERTa (Meta). Die Entwicklung in den letzten Jahren hat allerdings wahnsinnige Sprünge gemacht. Wer schon länger im Internet unterwegs ist, wird sicherlich schon, wenn auch nicht freiwillig, Erfahrungen mit Chatbots gemacht haben. Diese werden oft im Support oder auf Webseiten zur Kundenberatung eingesetzt. Natürlich sind Chatbots nicht mit den heutigen LLMs vergleichbar. Die Bots sind nicht lernfähig, können Texte nur schlecht analysieren und haben eine begrenzte Anzahl an Textbausteinen für Antworten. Ein interessanter Vertreter von Chatbots, der viral ging, war das "Subservient Chicken" von Burger King, das 2003/2004 veröffentlicht wurde (https://en.wikipedia.org/wiki/The_Subservient_Chicken). Dieser vergleiche zwischen Chatbots und heutigen LLM verdeutlicht aber wie gravierend die Entwicklung war.

Die Veröffentlichung von ChatGPT hat die Menschen gefühlt ziemlich überrascht. Vermutlich lag das daran, dass im Mainstream nicht bekannt war, wie weit die Entwicklung neuronaler Netze und künstlicher Intelligenz bereits fortgeschritten war und welche Möglichkeiten diese bieten. Dies dürfte auch teilweise den Erfolg und den Ansturm auf ChatGPT erklären. Mittlerweile werden solche LLMs für alles Mögliche genutzt: Sie erledigen Hausaufgaben,erstellen Quellcode, erzeugen Texte jeglicher Art oder verbessern und beantworten belanglose Fragen über den Sinn des Lebens. Damit lösen LLMs wie ChatGPT Suchmaschinen, Foren und sogar Social-Media-Plattformen wie Reddit teilweise ab. Nach dem Release konnte Stack Overflow bereits einen deutlichen Einbruch des Traffics feststellen [1]. Es bleibt offen, wie sich dies langfristig entwickeln wird. Es dürfte den Plattformen aber schwerfallen die Benutzer zurückzugewinnen.

Spannend ist die Frage, mit welchen Daten LLMs trainiert wurden. Schließlich beeinflusst dies die Qualität der Antworten. Grundsätzlich werden solche Modelle mit Daten aus vielen verschiedenen Quellen trainiert. Dazu gehören öffentlich verfügbare Texte und Dokumente, allgemein zugängliche Bücher oder lizenzierte Datensätze von Drittanbietern. Vereinfacht gesagt, kann ChatGPT sein Wissen aus allen frei verfügbaren Daten im Internet beziehen – seien es Dokumentationen, Blogeinträge, Forenbeiträge oder Social-Media-Posts. Einige Dienste hatten mit einer hohen Last zu kämpfen, die vermutlich von Crawlern für KI-Dienste verursacht wurde [2]. Es gibt mehrere Berichte von Administratoren, die feststellen, dass der Traffic von Bots stark gestiegen ist und teilweise sogar nun höher ist als der von menschlichen Nutzern [3]. Die Dead Internet Theory lässt grüßen (https://de.wikipedia.org/wiki/Dead_Internet_Theory). Auf GitHub gibt es bereits Projekte, die dabei helfen sollen, solche Crawler über robots.txt zu identifizieren und auszuschließen [4]. Dies wirft neue ethische Fragen auf. Auch wenn öffentliche Informationen für alle verfügbar sind, kann man die Ansicht vertreten, dass diese Verfügbarkeit von Organisationen ausgenutzt wird, um KI-Modelle zu trainieren und diese Services dann kommerziell anzubieten. Auf der anderen Seite gibt es natürlich keine größere Wissensdatenbank als das Internet selbst.

Wie gut die Antworten eines LLMs sind, ist sicherlich subjektiv. Generell kann man die Antworten aber als gut bis sehr gut oder als hilfreich bezeichnen. Allerdings liegt der Teufel im Detail. Ein mittlerweile bekanntes Phänomen sind Halluzinationen. Dabei liefert das LLM Antworten, die auf den ersten Blick richtig und plausibel erscheinen, aber tatsächlich falsch sind. Gründe hierfür können falsche oder fehlende Trainingsdaten, unzureichende Trainingsmethoden oder Probleme bei der Art und Weise sein, wie das Modell Inhalte erzeugt (Inferenz) [5]. Halluzinationen sind besonders schwer zu erkennen, wenn man von dem Thema kaum bis wenig Ahnung hat. Es gilt dann, besonders vorsichtig zu sein und die Informationen noch einmal zu überprüfen [6]. Natürlich gilt dies für alle Informationen aus dem Internet. Auch seriöse Quellen können fehlerhaft sein. Das Problem ist, dass wir dazu neigen, der KI zu sehr zu vertrauen, wenn wir bereits erste positive Erfahrungen gemacht haben. Das gleiche gilt natürlich auch für Informationen aus Foren oder anderen Quellen.

Ein weiterer Kritikpunkt ist, dass wir die KI zwar für schnelle Antworten nutzen, aber aus den Antworten häufig nichts lernen. Die Nutzung der KI zeigt, dass wir etwas nicht wissen, aber gerne schnell eine Antwort hätten. Daran ist zunächst nichts verwerflich, aber anstatt die Lösung einfach zu übernehmen und sich zu freuen, dass der Code funktioniert, sollte man versuchen, die Lösung zu verstehen und nachzuvollziehen. Nur so können wir aus der Nutzung von KI das Maximum herausholen. Dabei kann es schon reichen, dass die KI einem die Antwort noch einmal kurz erklärt.

Nicht zu unterschätzen ist auch, dass man der KI oft mehr vertraut als sich selbst oder den eigenen Fähigkeiten. Man sollte eine KI nicht über die eigene gesammelte Erfahrung stellen. Wir sollten nicht unsere wertvolle, jahrelange Erfahrung als Engineer, Softwareentwickler etc. vernachlässigen und völlig auf die KI setzen. Dieses Wissen und diese Erfahrung sind etwas, das eine KI nicht ersetzen kann.

Bei der Benutzung zeigt sich relativ schnell, was LLMs wie ChatGPT und Co. gut und eher nicht so gut können. Das Erzeugen von Quellcode – egal ob Bash, Python, Perl oder Go – funktioniert erstaunlich gut, zumindest bei leichten bis mittelkomplexen Aufgaben. Der Quellcode ist meist sehr sauber und orientiert sich an den entsprechenden Best Practices. Je genauer man die Anforderungen formuliert, desto besser wird der Quellcode. Beispielsweise validiert ChatGPT nicht immer Eingaben, erstellt Quellcode, der sehr fehlertolerant ist, oder baut bestimmte Codefragmente als Funktionen, damit diese wiederverwendet werden können. So etwas muss man ChatGPT in den meisten Fällen explizit sagen. Ich kann also mit ChatGPT guten Quellcode erstellen, muss aber genau wissen, was ich will und welche wichtigen Funktionen enthalten sein sollten.

Ebenfalls richtig gut ist ChatGPT bei der Korrektur von Texten. Grammatik und Rechtschreibung werden meist fehlerfrei verbessert, und wiederholende Wörter werden durch Alternativen ersetzt. Worin ChatGPT aber eher mäßig ist, ist das Erzeugen von Texten. Es ist natürlich faszinierend zu sehen, dass ChatGPT über fast jedes Thema einen Text schreiben kann. Allerdings wirken diese Texte meist sehr generisch und inhaltslos. Vom persönlichen Gefühl her würde ich sagen, dass man längere Texte noch unterscheiden kann, ob sie von einer KI oder einem Menschen geschrieben wurden – zumindest aktuell noch.

Ebenfalls eher gemischt ist ChatGPT beim Erstellen von Konfigurationen. Hier habe ich oft widersprüchliche Konfigurationsvorschläge erhalten, oder Parameter wurden nicht korrekt erklärt. Auch fällt es ChatGPT manchmal schwer, zwischen älteren und neueren Versionen zu unterscheiden. In einigen wenigen Fällen hat ChatGPT sogar fast wortwörtlich den Wortlaut aus der offiziellen Dokumentation des Herstellers übernommen.

Mir ist bewusst, dass meine gesammelten Erfahrungen über LLMs sehr subjektiv sind und dass andere Leute vermutlich andere Erfahrungen gemacht haben. Trotzdem ist und bleibt es spannend, die Entwicklung von KIs und LLMs zu verfolgen. Besonders interessant sind auch Funktionen wie der "Operator" von ChatGPT, der mit Webseiten interagieren kann. Wie bei jeder neuen Technologie sollten wir diese aber auch kritisch hinterfragen und nicht blind nutzen. Vor allem sensible Informationen wie firmeninterne Daten oder personenbezogene Daten sollten nicht mit einer KI geteilt werden. Man sollte sich auch bewusst sein, dass Organisationen diese Informationen möglicherweise speichern – wenn auch nur, um die Antworten zu bewerten.

P.S.: Wer sich dafür interessiert, wie KIs im Allgemeinen funktionieren, kann sich diesen guten Vortrag "Wie funktioniert ChatGPT ganz genau?" von Michael Christen auf der FrosCon 2024 ansehen: https://media.ccc.de/v/froscon2024-3054-wie_funktioniert_chatgpt_ganz_genau.

Quellen:  
[1] https://www.golem.de/news/entwickler-forum-chatgpt-verursacht-traffic-einbrueche-bei-stack-overflow-2305-174120.html  
[2] https://www.linux-magazin.de/news/crawler-der-ki-dienste-stoeren-massiv-websites/  
[3] https://medium.com/@maxime_26729/the-rise-of-ai-crawling-bots-8e0bd8be6a36  
[4] https://github.com/ai-robots-txt/ai.robots.txt  
[5] https://www.iese.fraunhofer.de/blog/halluzinationen-generative-ki-llm/  
[6] https://www.golem.de/news/ki-erfindet-fallzitate-wieder-blamiert-sich-ein-anwalt-wegen-chatgpt-2502-192968.html
